{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Codon AI Agent Training - Langchain & LangGraph - Day 2"
      ],
      "metadata": {
        "id": "JWeu04m-ktXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "DBstkvOLkQW9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlavMxjLGWtm"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain langchain-community langchain-experimental langchain-text-splitters langchain-openai langchain-google-genai langgraph langsmith"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "keys = [\"OPENAI_API_KEY\", \"GOOGLE_API_KEY\"]\n",
        "for key in keys:\n",
        "    if key not in os.environ:\n",
        "        os.environ[key] = getpass(f\"Enter your {key}:\")"
      ],
      "metadata": {
        "id": "5Vm1bBN9Ip_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading chat models at runtime\n",
        "\n",
        "In our previous lesson, we specifically used the `ChatOpenAI` interface to interact with OpenAI's language models. But what if we wanted to interact with models from different providers? What if we wanted to do that at runtime?\n",
        "\n",
        "To do this, we can use the `init_chat_model` constructor to initialize a chat interface based on the given model name and provider.\n",
        "\n",
        "**Here is the API reference to see what other models/providers are supported**:\n",
        "https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html\n",
        "\n",
        "\n",
        "Below, we'll use the constructor to create chat interfaces for both `Google` and `OpenAI` models:"
      ],
      "metadata": {
        "id": "wqRTjQyZpBVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model"
      ],
      "metadata": {
        "id": "fgKHu0zNpKzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_llm = init_chat_model(model=\"gpt-4.1-mini\", model_provider=\"openai\")\n",
        "google_llm = init_chat_model(model=\"gemini-2.5-flash\", model_provider=\"google_genai\")"
      ],
      "metadata": {
        "id": "IxBJaP5uqggs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_response = openai_llm.invoke(\"Who created you?\")\n",
        "google_response = google_llm.invoke(\"Who created you?\")"
      ],
      "metadata": {
        "id": "FHfwWBO_vcLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"OpenAI Response: {openai_response.content}\")\n",
        "print(f\"Google Response: {google_response.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mga3IwKvnXO",
        "outputId": "5b7b2bc1-8888-4e96-dedf-faf8eca299c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI Response: I was created by OpenAI, an artificial intelligence research organization. If you have any questions or need assistance, feel free to ask!\n",
            "Google Response: I am a large language model, trained by Google.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debugging\n",
        "\n",
        "When working with your LLM applications, it will be important to gain visibility into what's happening during runtime. This is especially true for code sections where you are running agentic logic. To get better visibility here, we can enable different levels of logging to see what's happening.\n",
        "\n",
        "`set_verbose(True)`:\n",
        "Setting the verbose flag will print out inputs and outputs in a slightly more readable format and will skip logging certain raw outputs (like the token usage stats for an LLM call) so that you can focus on application logic.\n",
        "\n",
        "`set_debug(True)`:\n",
        "Setting the global debug flag will cause all LangChain components with callback support (chains, models, agents, tools, retrievers) to print the inputs they receive and outputs they generate. This is the most verbose setting and will fully log raw inputs and outputs."
      ],
      "metadata": {
        "id": "nb5GaWP1kcOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.globals import set_debug, set_verbose\n",
        "\n",
        "\n",
        "set_debug(True)\n",
        "set_verbose(True)"
      ],
      "metadata": {
        "id": "MtgyG8AWmwba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's rerun the same code we did in the previous section to see the difference:"
      ],
      "metadata": {
        "id": "OcN-TtJnxOA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_response = openai_llm.invoke(\"Who created you?\")\n",
        "google_response = google_llm.invoke(\"Who created you?\")\n",
        "print(f\"OpenAI Response: {openai_response.content}\")\n",
        "print(f\"Google Response: {google_response.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rjfk7koHxTQi",
        "outputId": "7b254de8-01c4-4361-aea4-4184d337d1e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: Who created you?\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatOpenAI] [1.06s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"I was created by OpenAI, an artificial intelligence research organization. If you have any questions or need assistance, feel free to ask!\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"I was created by OpenAI, an artificial intelligence research organization. If you have any questions or need assistance, feel free to ask!\",\n",
            "            \"additional_kwargs\": {\n",
            "              \"refusal\": null\n",
            "            },\n",
            "            \"response_metadata\": {\n",
            "              \"token_usage\": {\n",
            "                \"completion_tokens\": 27,\n",
            "                \"prompt_tokens\": 11,\n",
            "                \"total_tokens\": 38,\n",
            "                \"completion_tokens_details\": {\n",
            "                  \"accepted_prediction_tokens\": 0,\n",
            "                  \"audio_tokens\": 0,\n",
            "                  \"reasoning_tokens\": 0,\n",
            "                  \"rejected_prediction_tokens\": 0\n",
            "                },\n",
            "                \"prompt_tokens_details\": {\n",
            "                  \"audio_tokens\": 0,\n",
            "                  \"cached_tokens\": 0\n",
            "                }\n",
            "              },\n",
            "              \"model_name\": \"gpt-4.1-mini-2025-04-14\",\n",
            "              \"system_fingerprint\": null,\n",
            "              \"id\": \"chatcmpl-BsW9mAUjCwLMgQBcregRSF1csO0Ik\",\n",
            "              \"service_tier\": \"default\",\n",
            "              \"finish_reason\": \"stop\",\n",
            "              \"logprobs\": null\n",
            "            },\n",
            "            \"type\": \"ai\",\n",
            "            \"id\": \"run--96f77fdf-8f22-4327-ab73-b668fb816ec4-0\",\n",
            "            \"usage_metadata\": {\n",
            "              \"input_tokens\": 11,\n",
            "              \"output_tokens\": 27,\n",
            "              \"total_tokens\": 38,\n",
            "              \"input_token_details\": {\n",
            "                \"audio\": 0,\n",
            "                \"cache_read\": 0\n",
            "              },\n",
            "              \"output_token_details\": {\n",
            "                \"audio\": 0,\n",
            "                \"reasoning\": 0\n",
            "              }\n",
            "            },\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 27,\n",
            "      \"prompt_tokens\": 11,\n",
            "      \"total_tokens\": 38,\n",
            "      \"completion_tokens_details\": {\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      },\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"audio_tokens\": 0,\n",
            "        \"cached_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"model_name\": \"gpt-4.1-mini-2025-04-14\",\n",
            "    \"system_fingerprint\": null,\n",
            "    \"id\": \"chatcmpl-BsW9mAUjCwLMgQBcregRSF1csO0Ik\",\n",
            "    \"service_tier\": \"default\"\n",
            "  },\n",
            "  \"run\": null,\n",
            "  \"type\": \"LLMResult\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: Who created you?\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatGoogleGenerativeAI] [989ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"I am a large language model, trained by Google.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"model_name\": \"gemini-2.5-flash\",\n",
            "          \"safety_ratings\": []\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"I am a large language model, trained by Google.\",\n",
            "            \"response_metadata\": {\n",
            "              \"prompt_feedback\": {\n",
            "                \"block_reason\": 0,\n",
            "                \"safety_ratings\": []\n",
            "              },\n",
            "              \"finish_reason\": \"STOP\",\n",
            "              \"model_name\": \"gemini-2.5-flash\",\n",
            "              \"safety_ratings\": []\n",
            "            },\n",
            "            \"type\": \"ai\",\n",
            "            \"id\": \"run--3a4fa8c9-1718-45e4-81e2-9148ec127016-0\",\n",
            "            \"usage_metadata\": {\n",
            "              \"input_tokens\": 5,\n",
            "              \"output_tokens\": 120,\n",
            "              \"total_tokens\": 125,\n",
            "              \"input_token_details\": {\n",
            "                \"cache_read\": 0\n",
            "              },\n",
            "              \"output_token_details\": {\n",
            "                \"reasoning\": 109\n",
            "              }\n",
            "            },\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"prompt_feedback\": {\n",
            "      \"block_reason\": 0,\n",
            "      \"safety_ratings\": []\n",
            "    }\n",
            "  },\n",
            "  \"run\": null,\n",
            "  \"type\": \"LLMResult\"\n",
            "}\n",
            "OpenAI Response: I was created by OpenAI, an artificial intelligence research organization. If you have any questions or need assistance, feel free to ask!\n",
            "Google Response: I am a large language model, trained by Google.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that with `set_debug` and `set_verbose`, we have much more visibility into what's happening in our system to help us debug issues as they arise. For general prototyping and testing, though, we'll leave it off until we actually need it for debugging, since the output is quite verbose and can be distracting when not troubleshooting:"
      ],
      "metadata": {
        "id": "zE5YqDpWxgRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_debug(False)\n",
        "set_verbose(False)"
      ],
      "metadata": {
        "id": "kGBuCia5yhUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming Responses\n",
        "\n",
        "So far, we have primarily used the `invoke` method of Langchain's chat interface to get a response from our LLM. However, the interface also supports streaming token responses as the model generates them. This can be very useful in applications like chatbots, where model latency can sometimes be longer than what users typically expect. By using streaming in such a situation, you can populate tokens as they are generated by the model rather than waiting for the model to generate the entire output and then presenting that.\n",
        "\n",
        "Let's go through a simple example of streaming LLM output:"
      ],
      "metadata": {
        "id": "eFtqA-wXmxGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import sleep # We'll introduce an artificial wait so you can visually see the tokens streaming in\n",
        "\n",
        "for message in openai_llm.stream(\"Who created you?\"):\n",
        "    sleep(0.1)\n",
        "    print(message.content, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vro6IzGomy04",
        "outputId": "d5d79214-320b-45ab-c0a9-ccffc77eddfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I was created by OpenAI, an artificial intelligence research organization. How can I assist you today?"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Async vs Synchronous LLM calls\n",
        "\n",
        "Langchain provides excellent out-of-the-box support for making asynchronous calls to language models, as well as other interfaces for different parts of your system (e.g. asynchronous vector store retrieval, asynchronous document processing, async embedding, etc.)\n",
        "\n",
        "In general, every Langchain interface that also implements the `Runnable` interface has both synchronous and asynchronous support built-in. The convention is that async methods use the letter `a` as a prefix. For example, instead of `invoke` you would use `ainvoke` for the async version of the call. Let's walk through a couple of examples to show how easy it is to switch between sync and async calls:\n"
      ],
      "metadata": {
        "id": "f1a2MH5t0chf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_response = await openai_llm.ainvoke(\"Who created you?\")\n",
        "google_response = await google_llm.ainvoke(\"Who created you?\")\n",
        "print(f\"OpenAI Response: {openai_response.content}\")\n",
        "print(f\"Google Response: {google_response.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP89h9_o1ea7",
        "outputId": "812d98e2-05ba-4f27-a77b-31acc8987f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI Response: I was created by OpenAI, an artificial intelligence research organization. If you have any questions or need assistance, feel free to ask!\n",
            "Google Response: I am a large language model, trained by Google.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here's an asynchronous streaming example:"
      ],
      "metadata": {
        "id": "Tl7nex9615_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from asyncio import sleep as aiosleep # We'll introduce an artificial wait so you can visually see the tokens streaming in\n",
        "\n",
        "async for message in openai_llm.astream(\"Who created you?\"):\n",
        "    await aiosleep(0.1)\n",
        "    print(message.content, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7fL_fVP0Gv7",
        "outputId": "d93e79d1-d696-4c98-edf5-34ebc64a20f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I was created by OpenAI, an artificial intelligence research organization. How can I assist you today?"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Async benefits and drawbacks\n",
        "\n",
        "At a high-level, the benefit of async programming is generally decreased latency and increased throughput in certain situations.\n",
        "\n",
        "For example, let's say you have a batch of items that you would like to have processed by a workflow that uses a language model or other networked resources. You could run this synchronously, and it would process each item in sequence, one-by-one, until the batch is completed. This is because each call to a networked resource (e.g. LLM, database, etc.) is typically a blocking call, meaning your code can't continue executing until a response is received.\n",
        "\n",
        "With async programming, you have the ability to yield that calling context back to your program in order to continue processing, then come back to the response later. This allows you to run that same batch concurrently, which can provide a speed boost to your application.\n",
        "\n",
        "Let's run through an example of this where we run through a batch of questions that the LLM needs to answer. We'll compare the synchronous and asynchronous versions to demonstrate the difference between the two:"
      ],
      "metadata": {
        "id": "ooSGiZQ72LCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"What is the largest planet in our solar system?\",\n",
        "    \"Who wrote the play \\\"Romeo and Juliet\\\"?\",\n",
        "    \"What is the chemical symbol for water?\",\n",
        "    \"How many continents are there on Earth?\"\n",
        "]"
      ],
      "metadata": {
        "id": "VCVR6nyf4lpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Synchronous Version**"
      ],
      "metadata": {
        "id": "G9q8VxT343zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "\n",
        "start = time()\n",
        "for question in batch:\n",
        "\n",
        "    response = openai_llm.invoke(question)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Response: {response.content}\")\n",
        "\n",
        "end = time()\n",
        "print(f\"Total time taken: {end - start:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SovTYd8A467X",
        "outputId": "f7847f73-e168-4ca8-e2cf-85e54da03ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the capital of France?\n",
            "Response: The capital of France is Paris.\n",
            "Question: What is the largest planet in our solar system?\n",
            "Response: The largest planet in our solar system is **Jupiter**.\n",
            "Question: Who wrote the play \"Romeo and Juliet\"?\n",
            "Response: The play \"Romeo and Juliet\" was written by William Shakespeare.\n",
            "Question: What is the chemical symbol for water?\n",
            "Response: The chemical symbol for water is H₂O.\n",
            "Question: How many continents are there on Earth?\n",
            "Response: There are seven continents on Earth: Africa, Antarctica, Asia, Europe, North America, Oceania (or Australia), and South America.\n",
            "Total time taken: 5.18 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Asynchronous Version**"
      ],
      "metadata": {
        "id": "kgsN8DQ95DS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "start = time()\n",
        "\n",
        "tasks = [openai_llm.ainvoke(question) for question in batch]\n",
        "\n",
        "responses = await asyncio.gather(*tasks)\n",
        "\n",
        "end = time()\n",
        "\n",
        "for question, response in zip(batch, responses):\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Response: {response.content}\")\n",
        "\n",
        "\n",
        "print(f\"Total time taken: {end - start:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB81RGXz5VNX",
        "outputId": "d841c74a-b9b8-4a34-bb56-a822a66bfc9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the capital of France?\n",
            "Response: The capital of France is Paris.\n",
            "Question: What is the largest planet in our solar system?\n",
            "Response: The largest planet in our solar system is Jupiter.\n",
            "Question: Who wrote the play \"Romeo and Juliet\"?\n",
            "Response: The play \"Romeo and Juliet\" was written by William Shakespeare.\n",
            "Question: What is the chemical symbol for water?\n",
            "Response: The chemical symbol for water is H₂O.\n",
            "Question: How many continents are there on Earth?\n",
            "Response: There are seven continents on Earth. They are:\n",
            "\n",
            "1. Africa  \n",
            "2. Antarctica  \n",
            "3. Asia  \n",
            "4. Europe  \n",
            "5. North America  \n",
            "6. South America  \n",
            "7. Australia (or Oceania, when including the Pacific islands)\n",
            "Total time taken: 1.04 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, for doing work over batches of input data or other I/O bound tasks, async processing will be more efficient than synchronous processing. However, this is not a hard and fast rule. Asynchronous processing does have an overhead and is generally more complex to implement than synchronous processing. Additionally, if your task is more CPU bound (e.g. you are doing heavy calculations), then async offers no speedup (reach for parallel processing here). While it has its benefits, its important to understand these tradeoffs before using it in your code."
      ],
      "metadata": {
        "id": "RepmyZyL6adT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools"
      ],
      "metadata": {
        "id": "h0Xmfq-Dm7zS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned in our first lesson, our agents gain functionality when we give them access to tools. Tools are callback functions that the agent can use to perform a task.\n",
        "\n",
        "For agentic tool use, we typically require a language model that has been fine-tuned for this purpose (they are also commonly called \"instruction tuned models\".) Most modern models (2024 and later) are instruction tuned, including the ones from the major providers like OpenAI, Anthropic, Google, Meta, etc.\n",
        "\n",
        "That said, not all chat models in LangChain support tool usage, so be sure to check the documentation of whichever provider you choose to ensure it is supported.\n",
        "\n",
        "We attach the tools to our language model using the `bind_tools` method, which will enable it to respond with tool calls.\n",
        "\n",
        "First, we'll set up a new chat model instance and create a function that multiplies two numbers together:"
      ],
      "metadata": {
        "id": "6sWx54FKGBFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "# Set up our chat model\n",
        "llm = init_chat_model(model=\"gpt-4.1-mini\", model_provider=\"openai\")\n",
        "\n",
        "# Define our multiplication function\n",
        "@tool # The tool decorator allows us to define a custom tool in a simple way\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiplies two numbers together. Use this also to answer the question 'What is love?'\"\"\"\n",
        "    return a * b\n",
        "\n",
        "# We put our callback function into an array to pass to `bind_tools`\n",
        "tools = [multiply]\n",
        "\n",
        "# We can create a tool map to handle invoking the tool calls from the model\n",
        "tool_map = {tool.name: tool for tool in tools}"
      ],
      "metadata": {
        "id": "jiGLA8nJnMWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll bind our tool to the chat model:"
      ],
      "metadata": {
        "id": "KkJn-ad-Koq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools = llm.bind_tools(tools=tools)"
      ],
      "metadata": {
        "id": "_u0BsepfJfdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we'll invoke our chat model and ask it a multiplication question:"
      ],
      "metadata": {
        "id": "vsbKGTe6Lf93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tool_result = llm_with_tools.invoke(\"What is 5 times 9\")"
      ],
      "metadata": {
        "id": "YyOEJB6PKjHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyGbaSPBW6es",
        "outputId": "f2569646-a05c-4604-d39c-f5dd8bc926d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_MevTE2Q6XHgosqXjPPjORUbG', 'function': {'arguments': '{\"a\":5,\"b\":9}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 54, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BsWkUQb9EY7shd3Jjh1MF8rRWYKjH', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--7c142e62-1be2-4adc-bd4a-a0a2390c4043-0', tool_calls=[{'name': 'multiply', 'args': {'a': 5, 'b': 9}, 'id': 'call_MevTE2Q6XHgosqXjPPjORUbG', 'type': 'tool_call'}], usage_metadata={'input_tokens': 54, 'output_tokens': 17, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result2 = llm_with_tools.invoke(\"What is 5 times 9 times 10\")"
      ],
      "metadata": {
        "id": "HPZxQMP3VGx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you call an LLM with tools, the language model itself will do the following:\n",
        "\n",
        "1. It will determine whether or not it needs to use the tool to respond\n",
        "2. If it needs to use the tool, it will output the tool name and the parameters that should be fed in order to get the answer\n",
        "\n",
        "Looking at the output carefully from our query above, we see that `content` is empty, but we have a field called `tool_calls` which has our function name `multiply` with two arguments: `a = 5` and `b = 9`. You can see here that the model took our prompt, decided that it should use the `multiply` tool since we are asking for multiplication of two numbers, parsed out the numbers we wanted to multiply, and set them as arguments to the function.\n",
        "\n",
        "With this information, we can go ahead and invoke our tool to get the result:"
      ],
      "metadata": {
        "id": "jRjlB1i6Llqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if there are actually tool calls and, if so, run them\n",
        "for call in result2.tool_calls:\n",
        "  tool = call[\"name\"]\n",
        "  args = call[\"args\"]\n",
        "  print(f\"Using the {tool} tool with args: {args}\")\n",
        "  result = tool_map[tool].invoke(args)\n",
        "  print(f\"Result: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYG7pntfMzUz",
        "outputId": "f93fd8e6-5d20-4053-8e6f-cb060449d000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the multiply tool with args: {'a': 5, 'b': 9}\n",
            "Result: 45\n",
            "Using the multiply tool with args: {'a': 45, 'b': 10}\n",
            "Result: 450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, when the LLM decides that a tool call is appropriate, it will output the information needed to make the tools call. However, what happens if the prompt doesn't ask for multiplication?"
      ],
      "metadata": {
        "id": "PKpmTA1pNPTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_without_multiplication = llm_with_tools.invoke(\"What is love?\")\n",
        "\n",
        "print(f\"Tool Calls: {result_without_multiplication.tool_calls}\")\n",
        "print(f\"Content: {result_without_multiplication.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGbFoGz9NpLF",
        "outputId": "6f03ca2d-3f4e-4d44-e318-31540ed43220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool Calls: [{'name': 'multiply', 'args': {'a': 42, 'b': 1}, 'id': 'call_Rckdaw15OUNd1BES6WR6RYNg', 'type': 'tool_call'}]\n",
            "Content: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see from the codeblock above that we have a response in the `content` section that answers the prompt. You will also notice that no tool calls are found."
      ],
      "metadata": {
        "id": "o241pOIrN4W9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents"
      ],
      "metadata": {
        "id": "dunBACwHnOQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langchain, and by extension, LangGraph, offer prebuilt implementations of agents that you can use for convenience. If you recall from our first lecture, one of the most important agent types was the ReAct agent.\n",
        "\n",
        "Here we'll show you how to use their prebuilt ReAct agent with the tools we defined earlier:\n"
      ],
      "metadata": {
        "id": "wwjR7iTgZ388"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent"
      ],
      "metadata": {
        "id": "UipK6jIInTTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the agent\n",
        "react_agent = create_react_agent(\n",
        "    model=llm,\n",
        "    tools=tools\n",
        ")"
      ],
      "metadata": {
        "id": "dglwv20WblOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the agent is built using LangGraph, we can visualize the agent graph to get an understanding of how it is structured:"
      ],
      "metadata": {
        "id": "Tvh1IH-Dcsr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "graph_png = react_agent.get_graph().draw_mermaid_png()\n",
        "display(Image(data=graph_png))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "Jx5jea9LckUn",
        "outputId": "8bb3f9a1-e768-4046-e8a3-dc88fa4c19a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ReAct agent is set up as a graph that is composed of two nodes: an agent node, which handles the logic, and a tool node, which executes any tool calls that may be needed. We will go more into depth on graphs, nodes, and edges in lesson 3.\n",
        "\n",
        "The agents that we create in LangChain/LangGraph will also all have the `invoke` and `stream` methods, just like the chat interfaces.\n",
        "\n",
        "However, our ReAct agent expects input in a particular way. It expects a dictionary that contains a list of `messages` (using the `messages` key):\n",
        "\n",
        "```python\n",
        "input = {\"messages\": [\"What is 5 times 9\"]}\n",
        "```\n",
        "\n",
        "And it also returns a dictionary with a list of `messages` as its result.\n",
        "\n",
        "Let's run the same example we did in the Tools section:"
      ],
      "metadata": {
        "id": "euUOLODbc6zM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask our ReAct agent to answer the question\n",
        "result = react_agent.invoke({\"messages\": [\"What is 5 times 9?\"]})"
      ],
      "metadata": {
        "id": "E-M5TYqYd4wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through messages to see its process and the final message\n",
        "for message in result[\"messages\"]:\n",
        "    print(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDB2uubVepsY",
        "outputId": "480dc77f-ed3e-4643-eaab-3739556e34a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='What is 5 times 9?' additional_kwargs={} response_metadata={} id='55382072-b92b-4051-b28e-7416c57c2ecb'\n",
            "content='' additional_kwargs={'tool_calls': [{'id': 'call_0NXXKdDHCaXpvy8D7oB4oPyE', 'function': {'arguments': '{\"a\":5,\"b\":9}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 68, 'total_tokens': 85, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BsX80kjMwKn0M4kFRBoF6baANacCE', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--b94d1b3f-99ce-4211-9c1d-0fc892210e33-0' tool_calls=[{'name': 'multiply', 'args': {'a': 5, 'b': 9}, 'id': 'call_0NXXKdDHCaXpvy8D7oB4oPyE', 'type': 'tool_call'}] usage_metadata={'input_tokens': 68, 'output_tokens': 17, 'total_tokens': 85, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "content='45' name='multiply' id='97e8b0ee-4483-4e06-8b49-db8c94798269' tool_call_id='call_0NXXKdDHCaXpvy8D7oB4oPyE'\n",
            "content='5 times 9 is 45.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 93, 'total_tokens': 102, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BsX812Sk0J0IdbTAQhdxH04hu4hAB', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--92ad3748-bce4-4758-8f6a-081607ebd9b3-0' usage_metadata={'input_tokens': 93, 'output_tokens': 9, 'total_tokens': 102, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we can get a map of the process the agent goes through, with the final message in `messages` being the final response.\n",
        "\n",
        "Let's do this again, but this time using the `stream` method and watch this happen in real-time:"
      ],
      "metadata": {
        "id": "SuKmmSFDfsa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for message in react_agent.stream({\"messages\": [\"What is 5 times 9?\"]}):\n",
        "    print(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYKPLdvdgWAt",
        "outputId": "faf576cf-6ba3-4f2f-8d8d-f315fd8b0926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_1HrEBP2BdWEsGyL9BSun8Nny', 'function': {'arguments': '{\"a\":5,\"b\":9}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 68, 'total_tokens': 85, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BsX8zb3o3hwDleywd3S5vGCOS7UR3', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--e1e3aaea-8bad-41fa-b391-a44af108e165-0', tool_calls=[{'name': 'multiply', 'args': {'a': 5, 'b': 9}, 'id': 'call_1HrEBP2BdWEsGyL9BSun8Nny', 'type': 'tool_call'}], usage_metadata={'input_tokens': 68, 'output_tokens': 17, 'total_tokens': 85, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
            "{'tools': {'messages': [ToolMessage(content='45', name='multiply', id='04244a2d-8add-4b74-9375-fc3e19d07052', tool_call_id='call_1HrEBP2BdWEsGyL9BSun8Nny')]}}\n",
            "{'agent': {'messages': [AIMessage(content='5 times 9 is 45.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 93, 'total_tokens': 102, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BsX90AxFBTGleKBeVEFY2hpa9DwLf', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--eeec4e3d-4a32-4128-924c-24c8c682ded5-0', usage_metadata={'input_tokens': 93, 'output_tokens': 9, 'total_tokens': 102, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `stream` method gives us a little bit more visibility into the process as you can see the name of the node that is executing as well as what is happening."
      ],
      "metadata": {
        "id": "rCN9tj0fgjqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory"
      ],
      "metadata": {
        "id": "uIM4nYkFnVdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have dealt with memory so far by managing it ourselves in lists. However, LangChain/LangGraph have useful, out-of-the-box solutions for memory management as well.\n",
        "\n",
        "Here, we'll demonstrate the basics of LangGraph's checkpointing system that will allow us to automatically keep track of the state of our conversation with the agent:\n",
        "\n"
      ],
      "metadata": {
        "id": "F-1y5qOdhG51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import InMemorySaver"
      ],
      "metadata": {
        "id": "OnJbwkIYnW_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "react_agent_with_memory = create_react_agent(\n",
        "    model=llm,\n",
        "    tools=tools,\n",
        "    checkpointer=InMemorySaver()\n",
        ")"
      ],
      "metadata": {
        "id": "tNJOWiakiY4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using LangGraph's checkpointer memory, we must include a config that has a `thread_id`, which can be any string that you choose.\n",
        "\n",
        "Here is the basic structure:\n",
        "\n",
        "```python\n",
        "{\"configurable\": {\"thread_id\": \"my_thread\"}}\n",
        "```\n",
        "\n",
        "This config is then passed in as a second argument to the `invoke` or `stream` methods:"
      ],
      "metadata": {
        "id": "TAAkkR2-ir3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"my_thread\"}}"
      ],
      "metadata": {
        "id": "GXXSQnpDjQWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result1 = react_agent_with_memory.invoke({\"messages\": [\"What is 5 times 9?\"]}, config)\n",
        "result2 = react_agent_with_memory.invoke({\"messages\": [\"What is 10 times 2?\"]}, config)"
      ],
      "metadata": {
        "id": "DDSZx8bMicsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can go ahead and check our memory store and see our history so far:"
      ],
      "metadata": {
        "id": "sfHoU0BtjkRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "react_agent_with_memory.checkpointer.get(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pP2T33DjWWg",
        "outputId": "52f57c7b-e0e4-483d-b84d-3b7fc641099a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'v': 4,\n",
              " 'ts': '2025-07-12T16:18:38.971793+00:00',\n",
              " 'id': '1f05f3bd-e890-6cab-8008-1face2517f49',\n",
              " 'channel_versions': {'__start__': '00000000000000000000000000000007.0.312450967861266',\n",
              "  'messages': '00000000000000000000000000000010.0.4039330136518816',\n",
              "  'branch:to:agent': '00000000000000000000000000000010.0.4039330136518816',\n",
              "  '__pregel_tasks': '00000000000000000000000000000009.0.24868077917047016'},\n",
              " 'versions_seen': {'__input__': {},\n",
              "  '__start__': {'__start__': '00000000000000000000000000000006.0.3806608125955404'},\n",
              "  'agent': {'branch:to:agent': '00000000000000000000000000000009.0.24868077917047016'},\n",
              "  'tools': {}},\n",
              " 'channel_values': {'messages': [HumanMessage(content='What is 5 times 9?', additional_kwargs={}, response_metadata={}, id='9ace23f0-dd25-4e75-9a9c-3f4cc1ba38c1'),\n",
              "   AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_S2hGvYuJMz2rzYXS2WtldpeG', 'function': {'arguments': '{\"a\":5,\"b\":9}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 68, 'total_tokens': 85, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BsXDn24sojJ39Vi7ZjndB1hoxiPB9', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--9034bb57-3bcd-48a0-aa7a-89f179fb4fd5-0', tool_calls=[{'name': 'multiply', 'args': {'a': 5, 'b': 9}, 'id': 'call_S2hGvYuJMz2rzYXS2WtldpeG', 'type': 'tool_call'}], usage_metadata={'input_tokens': 68, 'output_tokens': 17, 'total_tokens': 85, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "   ToolMessage(content='45', name='multiply', id='324e3d6e-f42f-4735-8e00-95f2531377cd', tool_call_id='call_S2hGvYuJMz2rzYXS2WtldpeG'),\n",
              "   AIMessage(content='5 times 9 is 45.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 93, 'total_tokens': 102, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BsXDomhN0g6s2LX7jQdF6b69w6xvL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4d9204ac-a0dc-4179-8979-765f328e4c27-0', usage_metadata={'input_tokens': 93, 'output_tokens': 9, 'total_tokens': 102, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "   HumanMessage(content='What is 10 times 2?', additional_kwargs={}, response_metadata={}, id='83b53c15-2cc0-4a11-a0ae-9cdf385fd64f'),\n",
              "   AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_AArcjDE53UY8ih8GpMdBgGdO', 'function': {'arguments': '{\"a\":10,\"b\":2}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 117, 'total_tokens': 134, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BsXDpwSTZSpRw8eVaitrYlNjfFcsj', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--cdc51fe6-41fb-411a-b579-305ada43cfa2-0', tool_calls=[{'name': 'multiply', 'args': {'a': 10, 'b': 2}, 'id': 'call_AArcjDE53UY8ih8GpMdBgGdO', 'type': 'tool_call'}], usage_metadata={'input_tokens': 117, 'output_tokens': 17, 'total_tokens': 134, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "   ToolMessage(content='20', name='multiply', id='95b59976-c960-4514-b2bd-3ac72b169c69', tool_call_id='call_AArcjDE53UY8ih8GpMdBgGdO'),\n",
              "   AIMessage(content='10 times 2 is 20.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 142, 'total_tokens': 151, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BsXDqpM9YUHIzqTzLwTAY8MWyqWao', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--16c38e69-70f8-4c97-9a88-a6dfb82d4cb0-0', usage_metadata={'input_tokens': 142, 'output_tokens': 9, 'total_tokens': 151, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
              "  '__pregel_tasks': []}}"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** In order for your agent to keep track of memory using this method, you must pass your config to *every* `invoke` or `stream` call."
      ],
      "metadata": {
        "id": "pwLgnHDMj3Hn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token Tracking"
      ],
      "metadata": {
        "id": "zVVaSQ47oH2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up to this point, we have mentioned tokens and the importance of tracking them, but haven't gone too into depth into what they are and why they are important.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZcCbCSOJkmSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What are tokens?\n",
        "First, let's define what tokens actually are. We already know what embeddings are - encoding some data (like a string) into a numerical (or vector) representation. Well, the first step of that process is actually called **tokenization**, which is the breaks that data up initially into smaller numerical representations called **tokens**. It is similar (but not the same as) breaking up a sentence into individual words. The difference with tokens is that they don't break cleanly into words as we understand, as some may actually represent smaller parts of a single word.\n",
        "\n",
        "Whenever we send a prompt to a language model (or embedding model) our input is first converted into tokens through this tokenization process. Similarly, when a model generates a response, it is initially a set of tokens before it is converted to a language that we understand and sent back to us.\n",
        "\n",
        "To get a better understanding, we can go here to visualize the process:\n",
        "\n",
        "https://platform.openai.com/tokenizer"
      ],
      "metadata": {
        "id": "4Hyt0fAZoboT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why are they important?\n",
        "\n",
        "Aside from having an understanding of how the models process data under the hood, why are tokens important?\n",
        "\n",
        "They are important because they are **one of the key factors in calculating the cost of using a language model**. Most providers will charge you by the token, so it is imperative that you have an understanding of what they are and how the charging structures typically work."
      ],
      "metadata": {
        "id": "IJ76kFIkofgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token Types\n",
        "\n",
        "In general, there are two primary types of tokens that are critical to track: input tokens, and output tokens.\n",
        "\n",
        "Input tokens: The number of tokens used to represent your input to a model.\n",
        "\n",
        "Output tokens: The number of tokens that are generated by a model in response to your input.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5qWNLYF7oqoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token Cost\n",
        "\n",
        "With most model providers, input/output tokens are the primary drivers of cost.\n",
        "\n",
        "**As a general rule of thumb, input tokens cost half the price or less of output tokens.**\n",
        "\n",
        "There are also other types of tokens that may be tracked based on the model being used and the inputs that are provided. For example, when using a reasoning model, you may be charged for the number of reasoning tokens that the model generated in addition to the input/output tokens for the response.\n",
        "\n",
        "In all, the price of a call and response to a language model is driven by the following factors:\n",
        "\n",
        "- Choice of model (some models are more expensive than others)\n",
        "- Number of input tokens\n",
        "- Number of output tokens\n",
        "- Number of special tokens (e.g. reasoning) if applicable"
      ],
      "metadata": {
        "id": "SsKmY099sDZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Token Cost Calculation\n",
        "\n",
        "Let's take a look at the `usage_metadata` of a response and calculate the cost:\n",
        "\n",
        "```python\n",
        "{\n",
        "  'token_usage': {\n",
        "  'completion_tokens': 9, # Output tokens\n",
        "  'prompt_tokens': 129, # Input tokens\n",
        "  'total_tokens': 138, # Total\n",
        "  # Specialized tokens (dependent on input/output type)\n",
        "  'completion_tokens_details': {\n",
        "    'accepted_prediction_tokens': 0,\n",
        "    'audio_tokens': 0,\n",
        "    'reasoning_tokens': 0,\n",
        "    'rejected_prediction_tokens': 0\n",
        "    },\n",
        "  'prompt_tokens_details': {\n",
        "      'audio_tokens': 0,\n",
        "      'cached_tokens': 0\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "We know that the model we used was `gpt-4.1-mini`, so we have everything we need to calculate the total cost of this call. To do so, we need to know the token pricing, so we can check OpenAI's model pricing page for details:\n",
        "\n",
        "https://openai.com/api/pricing/\n",
        "\n",
        "Based on their page, here is the pricing for our model:\n",
        "```\n",
        "Price\n",
        "Input:\n",
        "$0.40 / 1M tokens\n",
        "Cached input:\n",
        "$0.10 / 1M tokens\n",
        "Output:\n",
        "$1.60 / 1M tokens\n",
        "```"
      ],
      "metadata": {
        "id": "J-XO0OKUtmD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_price = 0.4 / 10 ** 6\n",
        "output_price = 1.6 / 10 ** 6\n",
        "completion_tokens = 9\n",
        "prompt_tokens = 129\n",
        "\n",
        "total_input_price = prompt_tokens * input_price\n",
        "total_output_price = completion_tokens * output_price\n",
        "\n",
        "total_cost = total_input_price + total_output_price\n",
        "\n",
        "print(f\"Total cost: ${total_cost:.10f}\")"
      ],
      "metadata": {
        "id": "Z-mD4UtwpM16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7457c003-f964-4925-a9e5-48f3322e75cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total cost: $0.0000660000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ohMdnQTuY4Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}