{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlavMxjLGWtm"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain langchain-community langchain-text-splitters langchain-openai langchain-chroma langchain-tavily wikipedia gradio arxiv pymupdf pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "api_keys = [\"OPENAI_API_KEY\", \"TAVILY_API_KEY\"]\n",
        "for key in api_keys:\n",
        "    os.environ[key] = getpass(f\"Enter your {key}:\")"
      ],
      "metadata": {
        "id": "5Vm1bBN9Ip_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cedb0812-819c-48d2-82cd-125be52e308d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OPENAI_API_KEY:··········\n",
            "Enter your TAVILY_API_KEY:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 1: Build a simple RAG QA Chatbot"
      ],
      "metadata": {
        "id": "p0kT7DsRJJrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ingest the documents\n",
        "# Create text chunks\n",
        "# Embed & store the chunks in the vector db\n",
        "# Set up chat backend & create function for basic chat loop\n",
        "# Set up gradio UI"
      ],
      "metadata": {
        "id": "GdOdPcpfJAKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Ingestion\n",
        "\n",
        "In this section we are going to download the 5 latest papers from ArXiv on the subject of AI. Then we will load the documents into memory as text pages for chunking and embedding."
      ],
      "metadata": {
        "id": "ImRe1kgwVygf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ingest documents from Arxiv\n",
        "import arxiv\n",
        "from langchain_community.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "IgddMse0LKiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef753b9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95a92425-830c-45fa-e8f6-e97d57cdffed"
      },
      "source": [
        "# Search for the 5 latest papers on AI\n",
        "search = arxiv.Search(\n",
        "    query=\"AI\",\n",
        "    max_results=5,\n",
        "    sort_by=arxiv.SortCriterion.SubmittedDate\n",
        ")\n",
        "\n",
        "results = list(search.results())\n",
        "\n",
        "# Download the papers\n",
        "for result in results:\n",
        "    print(f\"Downloading {result.title}...\")\n",
        "    result.download_pdf()\n",
        "    print(\"Done.\")\n",
        "\n",
        "print(\"All papers downloaded.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-367305166.py:8: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  results = list(search.results())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Achieving Hilbert-Schmidt Independence Under Rényi Differential Privacy for Fair and Private Data Generation...\n",
            "Done.\n",
            "Downloading Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval...\n",
            "Done.\n",
            "Downloading Reasoning-Intensive Regression...\n",
            "Done.\n",
            "Downloading Operational Validation of Large-Language-Model Agent Social Simulation: Evidence from Voat v/technology...\n",
            "Done.\n",
            "Downloading From Drone Imagery to Livability Mapping: AI-powered Environment Perception in Rural China...\n",
            "Done.\n",
            "All papers downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdfs_from_directory(directory_path):\n",
        "    \"\"\"\n",
        "    Loads all PDF files from a directory and returns a dictionary\n",
        "    where keys are document titles and values are lists of pages.\n",
        "    \"\"\"\n",
        "    pdf_documents = {}\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            try:\n",
        "                loader = PyPDFLoader(file_path)\n",
        "                pages = loader.load_and_split()\n",
        "                pdf_documents[filename] = pages\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {filename}: {e}\")\n",
        "    return pdf_documents\n",
        "\n",
        "# Example usage (assuming your PDFs are in /content)\n",
        "pdf_data = load_pdfs_from_directory(\"/content\")"
      ],
      "metadata": {
        "id": "09XcrlYhS7WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_data.keys()"
      ],
      "metadata": {
        "id": "z3yXRVxHUBwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb833aaf-0e5d-47cd-bc97-fe6b30431e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['2508.21740v1.Operational_Validation_of_Large_Language_Model_Agent_Social_Simulation__Evidence_from_Voat_v_technology.pdf', '2508.21788v1.Going_over_Fine_Web_with_a_Fine_Tooth_Comb__Technical_Report_of_Indexing_Fine_Web_for_Problematic_Content_Search_and_Retrieval.pdf', '2508.21815v1.Achieving_Hilbert_Schmidt_Independence_Under_Rényi_Differential_Privacy_for_Fair_and_Private_Data_Generation.pdf', '2508.21762v1.Reasoning_Intensive_Regression.pdf', '2508.21738v1.From_Drone_Imagery_to_Livability_Mapping__AI_powered_Environment_Perception_in_Rural_China.pdf'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Break each document down into \"chunks\"\n",
        "\n",
        "**Explanation & Motivation**\n",
        "\n",
        "Now that we have our documents ingested and loaded into memory, we can begin the process of breaking them down into chunks.\n",
        "\n",
        "Chunking is used as a means to manage the amount of context fed into a language model during inference. This is especially useful when we want to use large documents as context.\n",
        "\n",
        "Another goal of chunking is to keep relevant context together in these smaller pieces. Language models tend to struggle with picking out details in larger text blocks, so the goal of our retrieval phase in our RAG pipeline is to only gather the most relevant chunks for the given query."
      ],
      "metadata": {
        "id": "idbw25DfXU_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "zHcXtRD1UHRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up your text_splitter here\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=250,\n",
        "    chunk_overlap=50\n",
        ") # Implement the text splitter using RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "bllzYEzyUoDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create chunks for each document. Remember, each document is also composed of one or more documents itself.\n",
        "# You can try using the `transform_documents` method to help process lists of documents\n",
        "\n",
        "chunks = {\n",
        "    title: text_splitter.transform_documents(docs)\n",
        "    for title, docs in pdf_data.items()\n",
        "}"
      ],
      "metadata": {
        "id": "lL0PcdH3asdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets observe the results of our chunking:"
      ],
      "metadata": {
        "id": "inz27yIscqfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_titles = list(chunks.keys())\n",
        "\n",
        "for title in doc_titles:\n",
        "  num_pages = len(pdf_data[title])\n",
        "  num_chunks = len(chunks[title])\n",
        "  print(f\"The document, {title.split('.')[-2]}, has {num_pages} pages that are split into {num_chunks} chunks.\")"
      ],
      "metadata": {
        "id": "i9IbEvznbRyo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3793b1b-3de5-4d1f-e7d7-68529c1a2023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document, Operational_Validation_of_Large_Language_Model_Agent_Social_Simulation__Evidence_from_Voat_v_technology, has 28 pages that are split into 375 chunks.\n",
            "The document, Going_over_Fine_Web_with_a_Fine_Tooth_Comb__Technical_Report_of_Indexing_Fine_Web_for_Problematic_Content_Search_and_Retrieval, has 28 pages that are split into 369 chunks.\n",
            "The document, Achieving_Hilbert_Schmidt_Independence_Under_Rényi_Differential_Privacy_for_Fair_and_Private_Data_Generation, has 27 pages that are split into 454 chunks.\n",
            "The document, Reasoning_Intensive_Regression, has 28 pages that are split into 395 chunks.\n",
            "The document, From_Drone_Imagery_to_Livability_Mapping__AI_powered_Environment_Perception_in_Rural_China, has 37 pages that are split into 357 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embed and store the chunks\n",
        "\n",
        "Next we'll use an embedding model to create vector representations of our chunks that can then be stored in our vector database. These vectors will be used during the retrieval phase of our RAG system where we will perform similarity search to find the most relevant chunks based on the given question."
      ],
      "metadata": {
        "id": "jX7yH8GleOXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma"
      ],
      "metadata": {
        "id": "9rcxCVIIbYq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize embeddings from OpenAI\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ],
      "metadata": {
        "id": "3LLAh9Aubgol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = Chroma(\n",
        "    collection_name=\"lc-demo\",\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=\"/content/lc-vector-store\"\n",
        ")"
      ],
      "metadata": {
        "id": "SCJDzPZlgq6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have initialized our embeddings and vector store, we're ready to embed and load our documents in:"
      ],
      "metadata": {
        "id": "EKXlKxcAiG3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for title, chunk_sequence in chunks.items():\n",
        "  # Add the method to add documents to your vector_store here\n",
        "  vector_store.add_documents(documents=chunk_sequence)\n",
        "  print(f\"Added chunks for {title.split('.')[2]}\")"
      ],
      "metadata": {
        "id": "IUraS3iohp43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd25238-03a2-4dde-c530-1b57c274a84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added chunks for Operational_Validation_of_Large_Language_Model_Agent_Social_Simulation__Evidence_from_Voat_v_technology\n",
            "Added chunks for Going_over_Fine_Web_with_a_Fine_Tooth_Comb__Technical_Report_of_Indexing_Fine_Web_for_Problematic_Content_Search_and_Retrieval\n",
            "Added chunks for Achieving_Hilbert_Schmidt_Independence_Under_Rényi_Differential_Privacy_for_Fair_and_Private_Data_Generation\n",
            "Added chunks for Reasoning_Intensive_Regression\n",
            "Added chunks for From_Drone_Imagery_to_Livability_Mapping__AI_powered_Environment_Perception_in_Rural_China\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the chain to invoke the LLM"
      ],
      "metadata": {
        "id": "cQa8IpE2mjWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.output_parsers.string import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from typing import List, Dict, Any\n",
        "import time"
      ],
      "metadata": {
        "id": "8FEqNYPOmmQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0) # Set up the chat model here"
      ],
      "metadata": {
        "id": "z1IHmrlnnAy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can use this as a base prompt and modify it if you feel you need to\n",
        "system_prompt = \"\"\"\n",
        "You are a helpful chatbot that answers questions about the subject of AI\n",
        "based on ONLY the context provided to you. Do not use any other context.\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content=system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{question} {context}\")\n",
        "])\n",
        "\n",
        "chain = prompt_template | llm | StrOutputParser() # Set up chain here"
      ],
      "metadata": {
        "id": "RWFlHu4NngaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"question\": \"What is the meaning of life, the universe, and everything?\", \"context\": \"\", \"chat_history\": []})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "pXpSnk900P0H",
        "outputId": "d0692ff7-cf04-43b5-b9c8-46766999be4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The context provided does not include information about the meaning of life, the universe, and everything. Therefore, I am unable to provide an answer based on the given information.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def arxiv_chat(question: str, history: List[Dict[str, Any]]):\n",
        "  retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
        "  context = retriever.invoke(question)\n",
        "  response = chain.invoke({\"question\": question, \"context\": context, \"chat_history\": history}) # Invoke the `chain` we created before and pass in the input with the following keys: `question`, `context`, and `chat_history`\n",
        "  # message = f\"{response.content}\\n\\nToken Usage: {response.response_metadata['token_usage']}\"\n",
        "  return response\n",
        "\n"
      ],
      "metadata": {
        "id": "8LlSYqJUoNsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the Gradio Chat UI"
      ],
      "metadata": {
        "id": "t1n2aU5HqATo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "XXixHNgzpAZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.ChatInterface(fn=arxiv_chat, title=\"ArXiv Chat\", type=\"messages\").launch(debug=True)"
      ],
      "metadata": {
        "id": "XG9flIfPqFvB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "outputId": "b53662ef-aa05-4bd8-876b-4becfb0c138d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://435d90dbf5bf2dad4e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://435d90dbf5bf2dad4e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}