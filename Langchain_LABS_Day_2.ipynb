{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VlavMxjLGWtm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: typer 0.17.4 does not provide the extra 'all'\n"
          ]
        }
      ],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain langchain-community langchain-experimental langchain-text-splitters langchain-openai pydantic langchain-chroma langchain-tavily wikipedia gradio arxiv pymupdf pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vm1bBN9Ip_Y",
        "outputId": "0880d0e0-1c77-48dd-ce24-873ae70108d7"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "api_keys = [\"OPENAI_API_KEY\", \"TAVILY_API_KEY\"]\n",
        "for key in api_keys:\n",
        "    os.environ[key] = getpass(f\"Enter your {key}:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GhqPCRIy_ah"
      },
      "source": [
        "# Lab 2: Building a simple Research Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7zl6YmNdLWx"
      },
      "source": [
        "## Part A: Using the `PlanAndExecute` pre-built chain\n",
        "\n",
        "In this first section of the lab, we will implement the `PlanAndExecute` chain provided by Langchain and run it in our Gradio chat UI. We'll identify the strengths and weaknesses of this version, then look at implementing our own after!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZdNc1VOz3Ll"
      },
      "source": [
        "### Build the agent toolkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gvJmcM61z15R"
      },
      "outputs": [],
      "source": [
        "from langchain_tavily import TavilySearch\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_core.tools import tool\n",
        "from typing import Dict, Any, List\n",
        "from langchain_openai import ChatOpenAI\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w0_oJFAI0UiT"
      },
      "outputs": [],
      "source": [
        "# Tavily Search client\n",
        "tavily = TavilySearch(\n",
        "    max_results = 5,\n",
        "    topic = \"general\"\n",
        ")\n",
        "\n",
        "# Wikipedia client\n",
        "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9G-dkrvd06WV"
      },
      "outputs": [],
      "source": [
        "@tool(description=\"A tool that performs general web search using Tavily search engine\", parse_docstring=True)\n",
        "def tavily_search(query: str) -> Dict[str, Any]:\n",
        "  \"\"\"This tool performs a search using the Tavily Search engine.\n",
        "  It is useful for finding information on a given topic or query from\n",
        "  a number of sources across the web.\n",
        "\n",
        "  Args:\n",
        "    query: The query to search for\n",
        "  \"\"\"\n",
        "  return tavily.run(query)\n",
        "\n",
        "@tool(description=\"A tool that performs a search on Wikipedia using the Wikipedia API\", parse_docstring=True)\n",
        "def wikipedia_search(query: str) -> Dict[str, Any]:\n",
        "  \"\"\"This tool performs a search on Wikipedia using the Wikipedia API. It is useful for finding information on a given topic or query from Wikipedia.\n",
        "\n",
        "  Args:\n",
        "    query: The query to search for\n",
        "  \"\"\"\n",
        "  return wikipedia.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VizH4sWd13hP"
      },
      "outputs": [],
      "source": [
        "toolkit = [tavily_search, wikipedia_search]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmwSQ7rN5vor"
      },
      "source": [
        "### Set up the agent chains\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7PDzHSLy9wvA"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.plan_and_execute.agent_executor import PlanAndExecute\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FLR7_7Nq7CKR"
      },
      "outputs": [],
      "source": [
        "#\n",
        "planner_llm = ChatOpenAI(model=\"gpt-4.1-nano\")\n",
        "executor_llm = ChatOpenAI(model=\"gpt-4.1-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzllWVrNAzAq"
      },
      "source": [
        "### (Optional) Add a system prompt\n",
        "\n",
        "**NOTE:** For the Plan and Execute agent, our system prompt must help the Planner chain output a sequence of steps that can then be parsed by the Executor chain. In order to do this, we must explicitly write these instructions into the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bTPcASeKCRy8"
      },
      "outputs": [],
      "source": [
        "planner_prompt = \"\"\"\n",
        "You are a planner who is an expert at coming up with a plan to\n",
        "answer the following questions as best you understand. Think things\n",
        "through step-by-step and output your step-by-step plan as a sequence of steps.\n",
        "Use the tools that you have at your disposal to answer questions.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xGMnqA3p8jZH"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.plan_and_execute.planners.chat_planner import load_chat_planner\n",
        "from langchain_experimental.plan_and_execute.executors.agent_executor import load_agent_executor\n",
        "\n",
        "\n",
        "planner = load_chat_planner(\n",
        "    llm=planner_llm,\n",
        "    system_prompt=planner_prompt,\n",
        ")\n",
        "\n",
        "executor = load_agent_executor(\n",
        "    llm=executor_llm,\n",
        "    tools=toolkit,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "agent = PlanAndExecute(\n",
        "    executor=executor,\n",
        "    planner=planner,\n",
        "    verbose=True,\n",
        "    include_run_info=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnenAK18E5Im"
      },
      "source": [
        "### Set up the UI\n",
        "\n",
        "We'll use Gradio for our chat UI just like we did in our first lab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Wm4ScJ07svQj"
      },
      "outputs": [],
      "source": [
        "def research_chat(question: str, history: List[Dict[str, Any]]):\n",
        "  response = agent.invoke({\"input\": question, \"history\": history})\n",
        "  return response.get(\"output\", \"No response from agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bu7aG80H5qip",
        "outputId": "e91adcf4-c28d-4959-b905-09157865319c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Marcus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ChatInterface.__init__() got an unexpected keyword argument 'type'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatInterface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresearch_chat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mResearch Agent Chat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlaunch(debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;31mTypeError\u001b[0m: ChatInterface.__init__() got an unexpected keyword argument 'type'"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "gr.ChatInterface(fn=research_chat, title=\"Research Agent Chat\", type=\"messages\").launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB-eZjdaKP9A"
      },
      "source": [
        "### Current Limitations\n",
        "\n",
        "Out of the box, we get a ReAct style agent with rudimentary planning that can be used as a base for agentic applications. However, there are limitations to this implementation:\n",
        "\n",
        "- Blackbox planning: The planning layer is mostly obfuscated from us\n",
        "- Little control over agent behavior via prompts\n",
        "- Little to no control over the logging\n",
        "- Missing token usage or other useful metadata\n",
        "- Documentation is sparse\n",
        "\n",
        "While this might be ok for prototyping or very minimal applications, it is too brittle for anything substantial. The good thing is that we've learned enough to implement our OWN version of the Plan and Execute, ReAct style agent!\n",
        "\n",
        "We are going to do exactly that in the next part of this lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEKl3DJTLagC"
      },
      "source": [
        "## Part B: Implementing the Plan and Execute agent chain\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "We will build on what we've put together so far and, instead of using LangChain's implementations, we will build out a planner and executor, each as a modular chain. Then we will compose them together to create our ReAct style research agent with access to the Wikipedia and Tavily Search tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD-5XW4QOJrg"
      },
      "source": [
        "### The Planner Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNxjqb-kLWfr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"task\": \"Get hourly gym check-ins from Postgres, summarize peaks, and plot a bar chart.\",\n",
            "  \"assumptions\": [\n",
            "    \"Database credentials are stored in environment variables.\",\n",
            "    \"The database contains a table with gym check-in records including timestamps.\"\n",
            "  ],\n",
            "  \"steps\": [\n",
            "    {\n",
            "      \"id\": 1,\n",
            "      \"action\": \"query_db\",\n",
            "      \"description\": \"Connect to the Postgres database and retrieve hourly gym check-in data.\",\n",
            "      \"tool\": \"Postgres\",\n",
            "      \"inputs\": {\n",
            "        \"query\": \"SELECT date_trunc('hour', check_in_time) AS hour, COUNT(*) AS check_in_count FROM gym_check_ins GROUP BY hour ORDER BY hour;\"\n",
            "      },\n",
            "      \"success_criteria\": \"Hourly check-in data is successfully retrieved.\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": 2,\n",
            "      \"action\": \"summarize\",\n",
            "      \"description\": \"Analyze the retrieved data to identify peak check-in hours.\",\n",
            "      \"tool\": null,\n",
            "      \"inputs\": {\n",
            "        \"data\": \"Hourly check-in data from step 1.\"\n",
            "      },\n",
            "      \"success_criteria\": \"Peak hours are identified and summarized.\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": 3,\n",
            "      \"action\": \"visualize\",\n",
            "      \"description\": \"Create a bar chart to visualize the hourly check-in counts.\",\n",
            "      \"tool\": \"Matplotlib\",\n",
            "      \"inputs\": {\n",
            "        \"data\": \"Summarized peak check-in hours from step 2.\"\n",
            "      },\n",
            "      \"success_criteria\": \"A bar chart is generated and displayed.\"\n",
            "    }\n",
            "  ],\n",
            "  \"final_artifact\": \"Bar chart visualizing hourly gym check-ins.\",\n",
            "  \"notes\": null\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# === Planner Chain ===\n",
        "# Converts a natural-language task into a structured JSON plan.\n",
        "\n",
        "# %pip install -q langchain langchain-openai pydantic\n",
        "\n",
        "import os\n",
        "from typing import List, Optional, Literal\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "# ----- schema -----\n",
        "class PlanStep(BaseModel):\n",
        "    id: int = Field(..., description=\"1-based step index\")\n",
        "    action: Literal[\"search\",\"read\",\"call_api\",\"run_code\",\"query_db\",\"summarize\",\"analyze\",\"visualize\",\"other\"]\n",
        "    description: str\n",
        "    tool: Optional[str] = Field(None, description=\"explicit tool name if any\")\n",
        "    inputs: dict = Field(default_factory=dict)\n",
        "    success_criteria: str\n",
        "\n",
        "class Plan(BaseModel):\n",
        "    task: str\n",
        "    assumptions: List[str] = []\n",
        "    steps: List[PlanStep]\n",
        "    final_artifact: str\n",
        "    notes: Optional[str] = None\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Plan)\n",
        "\n",
        "MODEL_NAME = os.getenv(\"PLANNER_MODEL\",\"gpt-4o-mini\")\n",
        "llm_plan = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
        "\n",
        "system_txt = (\n",
        "    \"You are a precise AI Planner. Break the user's task into 3â€“7 reliable steps. \"\n",
        "    \"Specify tools and inputs whenever external actions are needed. \"\n",
        "    \"Do not invent credentials or endpoints; write assumptions instead. \"\n",
        "    \"Return ONLY valid JSON that matches the schema.\"\n",
        ")\n",
        "prompt_plan = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_txt),\n",
        "    (\"human\", \"Task:\\n{task}\\n\\nContext:\\n{context}\\n\\n{format_instructions}\")\n",
        "])\n",
        "fmt = parser.get_format_instructions()\n",
        "\n",
        "planner_chain = prompt_plan | llm_plan | parser\n",
        "\n",
        "def plan_task(task: str, context: str = \"\") -> Plan:\n",
        "    return planner_chain.invoke({\"task\": task.strip(), \"context\": context.strip(), \"format_instructions\": fmt})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6be9TmAONY1"
      },
      "source": [
        "### The Executor Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mWBsL4BiOOh4"
      },
      "outputs": [],
      "source": [
        "# === Executor Chain ===\n",
        "# Executes the steps from the planner with a small tool registry.\n",
        "\n",
        "import os, time, json, traceback\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, Optional, Callable, List\n",
        "\n",
        "# optional deps\n",
        "try:\n",
        "    import requests\n",
        "except Exception:\n",
        "    requests = None\n",
        "\n",
        "try:\n",
        "    import psycopg  # psycopg3\n",
        "except Exception:\n",
        "    psycopg = None\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "@dataclass\n",
        "class StepResult:\n",
        "    step_id: int\n",
        "    action: str\n",
        "    tool: Optional[str]\n",
        "    ok: bool\n",
        "    output: Any = None\n",
        "    error: Optional[str] = None\n",
        "    latency_s: float = 0.0\n",
        "\n",
        "@dataclass\n",
        "class ExecutionLog:\n",
        "    task: str\n",
        "    steps: List[StepResult] = field(default_factory=list)\n",
        "    def add(self, s: StepResult): self.steps.append(s)\n",
        "    @property\n",
        "    def ok(self): return all(s.ok for s in self.steps)\n",
        "    def to_json(self) -> str:\n",
        "        return json.dumps({\"task\": self.task, \"ok\": self.ok, \"steps\":[s.__dict__ for s in self.steps]}, indent=2)\n",
        "\n",
        "# ---- tools ----\n",
        "def t_call_api(inp: Dict[str,Any]) -> Any:\n",
        "    assert requests is not None, \"requests not installed\"\n",
        "    method = (inp.get(\"method\") or \"GET\").upper()\n",
        "    r = requests.request(method, inp[\"url\"], headers=inp.get(\"headers\"), params=inp.get(\"params\"),\n",
        "                         json=inp.get(\"json\"), data=inp.get(\"data\"), timeout=float(inp.get(\"timeout\",30)))\n",
        "    return {\"status\": r.status_code, \"headers\": dict(r.headers), \"text\": r.text[:20000]}\n",
        "\n",
        "def t_query_db(inp: Dict[str,Any]) -> Any:\n",
        "    assert psycopg is not None, \"psycopg not installed\"\n",
        "    dsn = dict(\n",
        "        host=os.getenv(\"PGHOST\",\"postgres\"),\n",
        "        port=int(os.getenv(\"PGPORT\",5432)),\n",
        "        dbname=os.getenv(\"PGDATABASE\",\"n8n\"),\n",
        "        user=os.getenv(\"PGUSER\",\"kura\"),\n",
        "        password=os.getenv(\"PGPASSWORD\",\"password\"),\n",
        "    )\n",
        "    with psycopg.connect(**dsn) as conn:\n",
        "        with conn.cursor() as cur:\n",
        "            cur.execute(inp[\"sql\"], inp.get(\"params\"))\n",
        "            if cur.description:\n",
        "                cols = [c.name for c in cur.description]\n",
        "                rows = cur.fetchall()\n",
        "                return {\"columns\": cols, \"rows\": rows}\n",
        "            return {\"rowcount\": cur.rowcount}\n",
        "\n",
        "def t_run_code(inp: Dict[str,Any]) -> Any:\n",
        "    code = inp[\"code\"]\n",
        "    g = {\"__builtins__\": __builtins__, \"time\": time}\n",
        "    l = {}\n",
        "    exec(code, g, l)  # for lab: do not run untrusted code in prod\n",
        "    return {\"locals\": {k: repr(v)[:1000] for k,v in l.items()}}\n",
        "\n",
        "def t_read(inp: Dict[str,Any]) -> Any:\n",
        "    assert requests is not None, \"requests not installed\"\n",
        "    r = requests.get(inp[\"url\"], timeout=float(inp.get(\"timeout\",20)))\n",
        "    return {\"status\": r.status_code, \"text\": r.text[:20000]}\n",
        "\n",
        "def _llm_small():\n",
        "    return ChatOpenAI(model=os.getenv(\"EXECUTOR_MODEL\",\"gpt-4o-mini\"), temperature=0)\n",
        "\n",
        "def t_summarize(inp: Dict[str,Any]) -> Any:\n",
        "    llm = _llm_small()\n",
        "    p = ChatPromptTemplate.from_messages([\n",
        "        (\"system\",\"Summarize precisely for a technical audience.\"),\n",
        "        (\"human\",\"Summarize:\\n\\n{c}\")\n",
        "    ])\n",
        "    msg = (p|llm).invoke({\"c\": inp.get(\"content\",\"\")})\n",
        "    return {\"summary\": msg.content}\n",
        "\n",
        "def t_analyze(inp: Dict[str,Any]) -> Any:\n",
        "    llm = _llm_small()\n",
        "    p = ChatPromptTemplate.from_messages([\n",
        "        (\"system\",\"Analyze and list 3â€“5 key findings + actions.\"),\n",
        "        (\"human\",\"Analyze:\\n\\n{c}\")\n",
        "    ])\n",
        "    msg = (p|llm).invoke({\"c\": inp.get(\"content\",\"\")})\n",
        "    return {\"analysis\": msg.content}\n",
        "\n",
        "def t_visualize(inp: Dict[str,Any]) -> Any:\n",
        "    import io, matplotlib\n",
        "    matplotlib.use(\"Agg\")\n",
        "    import matplotlib.pyplot as plt\n",
        "    x, y = inp[\"x\"], inp[\"y\"]\n",
        "    fig = plt.figure()\n",
        "    plt.plot(x, y)\n",
        "    plt.title(inp.get(\"title\",\"Plot\")); plt.xlabel(inp.get(\"xlabel\",\"x\")); plt.ylabel(inp.get(\"ylabel\",\"y\"))\n",
        "    buf = io.BytesIO(); fig.savefig(buf, format=\"png\", bbox_inches=\"tight\"); plt.close(fig); buf.seek(0)\n",
        "    return {\"image/png\": buf.getvalue()}\n",
        "\n",
        "TOOLBOX: Dict[str, Callable[[Dict[str,Any]], Any]] = {\n",
        "    \"call_api\": t_call_api,\n",
        "    \"query_db\": t_query_db,\n",
        "    \"run_code\": t_run_code,\n",
        "    \"search\": t_read, \"read\": t_read,\n",
        "    \"summarize\": t_summarize, \"analyze\": t_analyze,\n",
        "    \"visualize\": t_visualize,\n",
        "}\n",
        "\n",
        "def _exec_step(step: PlanStep) -> StepResult:\n",
        "    start = time.time()\n",
        "    try:\n",
        "        tool_name = step.tool or step.action\n",
        "        fn = TOOLBOX.get(tool_name)\n",
        "        if not fn: raise ValueError(f\"no tool for '{tool_name}'\")\n",
        "        out = fn(step.inputs or {})\n",
        "        return StepResult(step_id=step.id, action=step.action, tool=tool_name, ok=True, output=out, latency_s=time.time()-start)\n",
        "    except Exception as e:\n",
        "        return StepResult(step_id=step.id, action=step.action, tool=step.tool, ok=False,\n",
        "                          error=f\"{type(e).__name__}: {e}\\n{traceback.format_exc()}\",\n",
        "                          latency_s=time.time()-start)\n",
        "\n",
        "def execute_plan(plan: Plan) -> ExecutionLog:\n",
        "    log = ExecutionLog(task=plan.task)\n",
        "    for st in plan.steps:\n",
        "        res = _exec_step(st); log.add(res)\n",
        "        if not res.ok: break\n",
        "    return log\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0X56e-pOPRl"
      },
      "source": [
        "### Putting them together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Waw2yk1gORA_"
      },
      "outputs": [],
      "source": [
        "# === Planner + Executor together ===\n",
        "\n",
        "from IPython.display import JSON, display\n",
        "import json as _json\n",
        "\n",
        "def run_agentic_task(task: str, context: str = \"\"):\n",
        "    print(\"ðŸ”® planning...\")\n",
        "    plan = plan_task(task, context)\n",
        "    display(JSON(plan.model_dump(), expanded=False))\n",
        "\n",
        "    print(\"âš¡ executing...\")\n",
        "    log = execute_plan(plan)\n",
        "    display(JSON(_json.loads(log.to_json()), expanded=False))\n",
        "    return plan, log\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsCzhvgXORnZ"
      },
      "source": [
        "### Set up the UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tZ1lCEIyOTAn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: typer 0.17.4 does not provide the extra 'all'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMPORTANT: You are using gradio version 4.21.0, however version 4.44.1 is available, please upgrade.\n",
            "--------\n"
          ]
        }
      ],
      "source": [
        "# === Minimal Gradio UI ===\n",
        "%pip install -q gradio\n",
        "\n",
        "import gradio as gr, json, traceback\n",
        "\n",
        "def _extract_png(log_json: dict):\n",
        "    for s in log_json.get(\"steps\", []):\n",
        "        out = s.get(\"output\") or {}\n",
        "        if isinstance(out, dict) and \"image/png\" in out:\n",
        "            return out[\"image/png\"]\n",
        "    return None\n",
        "\n",
        "def ui_runner(task, context):\n",
        "    try:\n",
        "        plan = plan_task(task, context or \"\")\n",
        "    except Exception:\n",
        "        return (f\"PLANNER ERROR:\\n{traceback.format_exc()}\", \"\", None)\n",
        "    try:\n",
        "        log = execute_plan(plan)\n",
        "    except Exception:\n",
        "        return (json.dumps(plan.model_dump(), indent=2),\n",
        "                f\"EXECUTOR ERROR:\\n{traceback.format_exc()}\",\n",
        "                None)\n",
        "\n",
        "    plan_json = plan.model_dump()\n",
        "    log_json  = json.loads(log.to_json())\n",
        "    png = _extract_png(log_json)\n",
        "    return (json.dumps(plan_json, indent=2), json.dumps(log_json, indent=2), png)\n",
        "\n",
        "with gr.Blocks(title=\"Agentic Planner â†’ Executor\") as demo:\n",
        "    gr.Markdown(\"## planner â†’ executor (LangChain)\\nenter a task; iâ€™ll plan it and run it âœ¨\")\n",
        "    task_in    = gr.Textbox(label=\"Task\", lines=3, value=\"Summarize this text: 'Kura Labs agentic lab is running; list next actions.'\")\n",
        "    context_in = gr.Textbox(label=\"Context (optional)\", lines=3, value=\"No DB or external calls; use summarize/analyze.\")\n",
        "    run_btn    = gr.Button(\"Plan & Execute âš¡\", variant=\"primary\")\n",
        "    plan_out   = gr.Code(label=\"Plan (JSON)\", language=\"json\")\n",
        "    log_out    = gr.Code(label=\"Execution Log (JSON)\", language=\"json\")\n",
        "    img_out    = gr.Image(label=\"Visualization (if any)\", type=\"numpy\")\n",
        "    run_btn.click(ui_runner, [task_in, context_in], [plan_out, log_out, img_out])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QscZCC16OUZB"
      },
      "source": [
        "### Bonus Challenge:\n",
        "\n",
        "Now that you've built out your very own research agent, can you think of tools that you could add to enhance it? Check out the available tools from Langchain here: https://python.langchain.com/docs/integrations/tools/\n",
        "\n",
        "Or can you think of another type of application that you could build using the Plan and Execute chains and tools?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgU7Gzh5Oz92"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
